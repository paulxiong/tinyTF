{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9gOSiohu0PgvbbFYzOFYX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulxiong/tinyTF/blob/main/multi_words_mask_matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code randomly selects a percentage of tokens to mask, replaces them with [MASK], and then generates a masked word activity matrix using a learned feature creation matrix. The resulting matrix represents all possible pairs of words that occur with each token in the input sequence and is used during self-attention to ensure that each token can only attend to previous non-masked tokens in the sequence."
      ],
      "metadata": {
        "id": "l9kylJsWbH8w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "32lbfh-bakBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the input sequence and the percentage of tokens to mask\n",
        "input_sequence = \"the quick brown fox jumps over the lazy dog\"\n",
        "mask_percentage = 0.15\n",
        "\n",
        "# Split the input sequence into individual tokens\n",
        "tokens = input_sequence.split()\n",
        "\n",
        "# Determine how many tokens to mask based on the mask percentage\n",
        "num_to_mask = int(len(tokens) * mask_percentage)\n",
        "\n",
        "# Randomly select which tokens to mask\n",
        "mask_indices = torch.randperm(len(tokens))[:num_to_mask]\n",
        "\n",
        "# Replace the selected tokens with [MASK]\n",
        "for i in mask_indices:\n",
        "    tokens[i] = \"[MASK]\"\n",
        "\n",
        "# Define a feature creation matrix that maps each word to a vector representation\n",
        "feature_creation_matrix = torch.randn((len(tokens), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln4wxeQqhFmS",
        "outputId": "aa8bda0e-18f0-403b-a3e4-d1222365e6ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', '[MASK]']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_creation_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZyjmH2whPyG",
        "outputId": "de3b10d8-14e7-49e7-888c-6704bc80ebe7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3031,  3.8370],\n",
              "        [-0.9417, -0.1686],\n",
              "        [-0.4303,  0.3668],\n",
              "        [-0.1556, -1.4749],\n",
              "        [ 0.4633, -1.5158],\n",
              "        [-0.8226, -0.7534],\n",
              "        [ 1.3214,  0.6392],\n",
              "        [-0.3986, -1.6292],\n",
              "        [-1.5081, -1.8666]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty masked word activity matrix\n",
        "masked_word_activity_matrix = torch.zeros((len(tokens), len(tokens)))\n",
        "masked_word_activity_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPBLGM2ih-RA",
        "outputId": "e0188589-cb64-4c95-9b5c-6ff9a250853c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Iterate over each token in the sequence and generate features for all possible pairs of words that occur with that token\n",
        "for i, token in enumerate(tokens):\n",
        "    # Convert the token to its corresponding vector representation using the feature creation matrix\n",
        "    token_vector = feature_creation_matrix[i]\n",
        "    print(f\"token_vector= {token_vector}\\ntoken_vector.unsequeeze= {token_vector.unsqueeze(1)}\\ntoken_vector.sequeeze= {token_vector.unsqueeze(1).squeeze()}\")\n",
        "\n",
        "    # Multiply the token vector by the feature creation matrix to obtain a new vector that represents all possible pairs of words that occur with that word\n",
        "    pair_vectors = torch.matmul(feature_creation_matrix, token_vector.unsqueeze(1)).squeeze()\n",
        "    print(f\"pair_vectors= {pair_vectors} \\ntorch.matmul= {torch.matmul(feature_creation_matrix, token_vector.unsqueeze(1))}\")\n",
        "    # Set any entries corresponding to [MASK] tokens or future tokens to -infinity so they are not considered during self-attention\n",
        "    pair_vectors[:i] = float('-inf')\n",
        "    pair_vectors[i+1:] = float('-inf')\n",
        "    print(f\"pair_vectors=: {pair_vectors}\")\n",
        "\n",
        "    # Apply a softmax function to obtain a probability distribution over all possible pairs of words for this token\n",
        "    attention_scores = torch.softmax(pair_vectors, dim=0)\n",
        "    print(f'attention_scores= {attention_scores}')\n",
        "\n",
        "    # Store the attention scores in the mÂ asked word activity matrix\n",
        "    masked_word_activity_matrix[i] = attention_scores\n",
        "    print(f'masked_word_activity_matrix= {masked_word_activity_matrix}')\n",
        "\n",
        "# Print the resulting masked word activity matrix\n",
        "print(masked_word_activity_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz7YQcftg_iA",
        "outputId": "11e9bcf0-0e01-429a-afa2-22b1a0c5d3e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_vector= tensor([1.3031, 3.8370])\n",
            "token_vector.unsequeeze= tensor([[1.3031],\n",
            "        [3.8370]])\n",
            "token_vector.sequeeze= tensor([1.3031, 3.8370])\n",
            "pair_vectors= tensor([16.4209, -1.8742,  0.8468, -5.8622, -5.2125, -3.9628,  4.1747, -6.7708,\n",
            "        -9.1272]) \n",
            "torch.matmul= tensor([[16.4209],\n",
            "        [-1.8742],\n",
            "        [ 0.8468],\n",
            "        [-5.8622],\n",
            "        [-5.2125],\n",
            "        [-3.9628],\n",
            "        [ 4.1747],\n",
            "        [-6.7708],\n",
            "        [-9.1272]])\n",
            "pair_vectors=: tensor([16.4209,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "           -inf])\n",
            "attention_scores= tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-0.9417, -0.1686])\n",
            "token_vector.unsequeeze= tensor([[-0.9417],\n",
            "        [-0.1686]])\n",
            "token_vector.sequeeze= tensor([-0.9417, -0.1686])\n",
            "pair_vectors= tensor([-1.8742,  0.9152,  0.3433,  0.3953, -0.1806,  0.9017, -1.3522,  0.6501,\n",
            "         1.7349]) \n",
            "torch.matmul= tensor([[-1.8742],\n",
            "        [ 0.9152],\n",
            "        [ 0.3433],\n",
            "        [ 0.3953],\n",
            "        [-0.1806],\n",
            "        [ 0.9017],\n",
            "        [-1.3522],\n",
            "        [ 0.6501],\n",
            "        [ 1.7349]])\n",
            "pair_vectors=: tensor([  -inf, 0.9152,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-0.4303,  0.3668])\n",
            "token_vector.unsequeeze= tensor([[-0.4303],\n",
            "        [ 0.3668]])\n",
            "token_vector.sequeeze= tensor([-0.4303,  0.3668])\n",
            "pair_vectors= tensor([ 0.8468,  0.3433,  0.3197, -0.4741, -0.7553,  0.0776, -0.3341, -0.4261,\n",
            "        -0.0358]) \n",
            "torch.matmul= tensor([[ 0.8468],\n",
            "        [ 0.3433],\n",
            "        [ 0.3197],\n",
            "        [-0.4741],\n",
            "        [-0.7553],\n",
            "        [ 0.0776],\n",
            "        [-0.3341],\n",
            "        [-0.4261],\n",
            "        [-0.0358]])\n",
            "pair_vectors=: tensor([  -inf,   -inf, 0.3197,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-0.1556, -1.4749])\n",
            "token_vector.unsequeeze= tensor([[-0.1556],\n",
            "        [-1.4749]])\n",
            "token_vector.sequeeze= tensor([-0.1556, -1.4749])\n",
            "pair_vectors= tensor([-5.8622,  0.3953, -0.4741,  2.1997,  2.1636,  1.2393, -1.1484,  2.4651,\n",
            "         2.9878]) \n",
            "torch.matmul= tensor([[-5.8622],\n",
            "        [ 0.3953],\n",
            "        [-0.4741],\n",
            "        [ 2.1997],\n",
            "        [ 2.1636],\n",
            "        [ 1.2393],\n",
            "        [-1.1484],\n",
            "        [ 2.4651],\n",
            "        [ 2.9878]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf, 2.1997,   -inf,   -inf,   -inf,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([ 0.4633, -1.5158])\n",
            "token_vector.unsequeeze= tensor([[ 0.4633],\n",
            "        [-1.5158]])\n",
            "token_vector.sequeeze= tensor([ 0.4633, -1.5158])\n",
            "pair_vectors= tensor([-5.2125, -0.1806, -0.7553,  2.1636,  2.5122,  0.7610, -0.3568,  2.2849,\n",
            "         2.1307]) \n",
            "torch.matmul= tensor([[-5.2125],\n",
            "        [-0.1806],\n",
            "        [-0.7553],\n",
            "        [ 2.1636],\n",
            "        [ 2.5122],\n",
            "        [ 0.7610],\n",
            "        [-0.3568],\n",
            "        [ 2.2849],\n",
            "        [ 2.1307]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf,   -inf, 2.5122,   -inf,   -inf,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-0.8226, -0.7534])\n",
            "token_vector.unsequeeze= tensor([[-0.8226],\n",
            "        [-0.7534]])\n",
            "token_vector.sequeeze= tensor([-0.8226, -0.7534])\n",
            "pair_vectors= tensor([-3.9628,  0.9017,  0.0776,  1.2393,  0.7610,  1.2442, -1.5685,  1.5554,\n",
            "         2.6468]) \n",
            "torch.matmul= tensor([[-3.9628],\n",
            "        [ 0.9017],\n",
            "        [ 0.0776],\n",
            "        [ 1.2393],\n",
            "        [ 0.7610],\n",
            "        [ 1.2442],\n",
            "        [-1.5685],\n",
            "        [ 1.5554],\n",
            "        [ 2.6468]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf,   -inf,   -inf, 1.2442,   -inf,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([1.3214, 0.6392])\n",
            "token_vector.unsequeeze= tensor([[1.3214],\n",
            "        [0.6392]])\n",
            "token_vector.sequeeze= tensor([1.3214, 0.6392])\n",
            "pair_vectors= tensor([ 4.1747, -1.3522, -0.3341, -1.1484, -0.3568, -1.5685,  2.1547, -1.5681,\n",
            "        -3.1859]) \n",
            "torch.matmul= tensor([[ 4.1747],\n",
            "        [-1.3522],\n",
            "        [-0.3341],\n",
            "        [-1.1484],\n",
            "        [-0.3568],\n",
            "        [-1.5685],\n",
            "        [ 2.1547],\n",
            "        [-1.5681],\n",
            "        [-3.1859]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 2.1547,   -inf,   -inf])\n",
            "attention_scores= tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-0.3986, -1.6292])\n",
            "token_vector.unsequeeze= tensor([[-0.3986],\n",
            "        [-1.6292]])\n",
            "token_vector.sequeeze= tensor([-0.3986, -1.6292])\n",
            "pair_vectors= tensor([-6.7708,  0.6501, -0.4261,  2.4651,  2.2849,  1.5554, -1.5681,  2.8133,\n",
            "         3.6422]) \n",
            "torch.matmul= tensor([[-6.7708],\n",
            "        [ 0.6501],\n",
            "        [-0.4261],\n",
            "        [ 2.4651],\n",
            "        [ 2.2849],\n",
            "        [ 1.5554],\n",
            "        [-1.5681],\n",
            "        [ 2.8133],\n",
            "        [ 3.6422]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 2.8133,   -inf])\n",
            "attention_scores= tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "token_vector= tensor([-1.5081, -1.8666])\n",
            "token_vector.unsequeeze= tensor([[-1.5081],\n",
            "        [-1.8666]])\n",
            "token_vector.sequeeze= tensor([-1.5081, -1.8666])\n",
            "pair_vectors= tensor([-9.1272,  1.7349, -0.0358,  2.9878,  2.1307,  2.6468, -3.1859,  3.6422,\n",
            "         5.7584]) \n",
            "torch.matmul= tensor([[-9.1272],\n",
            "        [ 1.7349],\n",
            "        [-0.0358],\n",
            "        [ 2.9878],\n",
            "        [ 2.1307],\n",
            "        [ 2.6468],\n",
            "        [-3.1859],\n",
            "        [ 3.6422],\n",
            "        [ 5.7584]])\n",
            "pair_vectors=: tensor([  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 5.7584])\n",
            "attention_scores= tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "masked_word_activity_matrix= tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    }
  ]
}